---
title: "Auto-Stats"
author: "Arne John"
date: "`r Sys.Date()`"
output: html_document
# bibliography: ["references.bib"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("psych")
library("outliers")
library("car")
library("e1071")
library("tidyverse")
library("papaja")
library("citr")
```

```{r colorfunction, include=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

## How to Read This Document

Currently, this is a static html page for output. This document is a report that becomes the output when a person is doing an analysis on data. 

Different types of text: 

Regular text is the output found in the report, meant to be easily readable/understandable for the user.

SUPERSCRIPT DENOTES REQUIRED INPUTS or extra output not created. Addtionally, [Choices] are denoted in brackets for now.

`r colorize("Red font is meant to show missing areas as reminders for me to improve on.", "red")`


SELECT DATASET HERE. 
The independent Dataset is Kitchen Rolls; The paired Dataset is Moon and Aggression.
```{r data_input, eval=FALSE}
# This chunk is currently not being evaluated, as the data is being loaded in by the succeeding chunk. 
# Ultimately, the user should first upload a data file and then tell the program whether the data has paired samples or not. 
# The file is currently calling on two different datasets, depending on whether the user is analyzing paired or unpaired data.
data <- read.csv("kitchenRollsTTest.csv")
data_paired <- read.csv("Moon and Aggression.csv")
```

Is dataset paired? Y/N [Y]

Does your alternative hypothesis assume that the mean is different, greater, or lesser than the mean of the control group? [greater]

```{r poc_data_and_arguments, echo=FALSE}
# arguments
is_paired <- TRUE # TRUE or FALSE
alternative <- "greater" # c("two.sided", "less", "greater")

# From the arguments, the appropriate data file is loaded.
if (is_paired == TRUE) data_paired <- read.csv("Moon and Aggression.csv") else data <- read.csv("kitchenRollsTTest.csv")
```

```{r tidying_moon_data}
# Since the moon dataset uses two columns and so combines observations, we need to tidy the data so each row is its own observation rather than participant. 
data_paired["Participants"] <- rep(1:15)
data_paired <- pivot_longer(data = data_paired,
                            cols = Moon:Other,
                            names_to = "Condition",
                            values_to = "Outbursts")
```

SELECT DV AND IV.
[In this case, IV need to be pairs: Moon & Other]
[DV are all the cells responding to the columns, rows are participants]

```{r arguments, eval=FALSE}
# This code chunk represents the user's choices, but since that will likely be done through JASP, it contains no actual code. 
```

```{r calculation, include=FALSE}
ttest <- t.test(data = data_paired, Outbursts~Condition, var.equal = TRUE, alternative = alternative, paired = is_paired)
```

`r colorize('Maybe force the IV to be [,1] and the DV to be [,2] so the following steps can just refer to those columns? Otherwise generalize it so that following tests call upon the placeholder value. Though this wouldn not work as for paired data, specific columns would be selected to compare between.', "red")` Edit: Changed the file to fit requirements of paired tests.



## High Verbosity (HV)

### 1. Introduction

#### What are you achieving with this test?

Dear user,
You selected to conduct a Student's t-test. This test checks whether the data follows a Student's t-Distribution under the null hypothesis. It is often used to see whether the means of two sets of data are significantly different from one another. 

`r colorize('Based on what you entered like IV & DV. That would conceptually mean ou are testing this on this.', "red")`

#### What is Null-Hypothesis Significance Testing (NHST)?

Why are we doing this? It's a comparison between means?
But you could also just estimate parameters

This procedure tests whether the group means are equal, but also the size of the difference assumption it's not zero.

When researchers want to test whether some property or parameter has an effect on the distribution of observed data, they conduct a null hypothesis test. In this case, the null hypothesis assumes that the parameter influencing the distribution has no effect, i.e. it is equal to zero. In contrast, the alternative/statistical hypothesis calculates the size of the effect on the distribution.

Statistical significance is asserted via the p-value. This value calculated by the probability of obtaining a parameter that is at least as extreme as the observed parameter, assuming the null hypothesis to be correct/true. In other words, were one to repeat this experiment and collect data each time, the chance of getting a test result that is as high or higher as the observed result would only occur in p-value*100 percent of the time. Significance is assumed if the p-value falls below some previously asserted threshold value, usually set to 0.05, also known as the alpha level.

### 2. Descriptives

The data can be visualized with the help of a raincloud plot (Allen et al., 2019). This allows an immediate intuitive assessment of whether the groups differ, and whether the data in each group are approximately normally distributed.

`r colorize("INSERT RAINCLOUD PLOT HERE", "red")`

`r colorize("VISUALLY CONFIRMATION OF EQUAL VARIANCE BETWEEN DATA SETS & OUTLIERS", "red")`

Check for Model Misspecifications.

#### Assumption Checks

Before you can conduct a `r ttest$method`, we first need to check for assumptions that need to be held in order for the test to be valid. The assumptions for a `r ttest$method` are as follows:

- The two sets of data have similar variance.
- There are no significant outliers in the data that could influence the results.
- Neither data set contains skewness in the distribution of results.

`r colorize('Should independence be discussed? Can we test for independence in a meaningful way, or is it just meant to be thought-provoking?', "red")`

#### Equality of Variance / Homoscedasity

`r colorize("Paragraph explaining the necessity of equality of variance", "red")`
T-tests require both groups of data to have similar variance. If one 

```{r HV_homoscedasity_test, include = FALSE}
# Find Levene's test as code
leveneTest(sleep$extra, group = sleep$group)
levTest <- leveneTest(sleep$extra, group = sleep$group)
if (levTest$`Pr(>F)`[1] < 0.05) {
  levTestResult <- "The test showed significance, meeting that there is a difference in the variance of the two groups. That in turn means that the results of the t-test cannot be interpreted as valid. Instead of a Student's t-test, consider running a Welch's t-test which does not assume equal variance."
} else {
  levTestResult <- "The test did not show significance, meeting that the variances between the two groups is not significantly different. This means that you can safely conclude the results of the t-test."
}
```

To test homogeneity of variance, Levene's test was performed. `r levTestResult`

```{r HV_test_outliers, include=FALSE}
# dv_values <- as.matrix(data_paired[,3])
dixon <- dixon.test(as.matrix(data_paired[,3]))
grubbs <- grubbs.test(as.matrix(data_paired[,3]))
```

```{r HV_outliers_output, include=FALSE}
if (grubbs$p.value > 0.05 & dixon$p.value > 0.05) {
  outliers <- "Both tests reported no significant effect of outliers, so you can safely report the results of the test."} else if (grubbs$p.value < 0.05 & dixon$p.value > 0.05) {
  outliers <- paste0("Unfortunately, Grubb's test for outliers found a significance of the greatest outlier, G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should exclude the outlier. Don't forget to mention this in the report!")
  } else if (grubbs$p.value > 0.05 & dixon$p.value < 0.05) {
  outliers <- paste0("Unfortunately, Dixon's Q test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
  } else {
  outliers <- paste0("Unfortunately, both the Dixon's Q test and the Grubb's test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
}
```


#### Outliers

`r colorize("Do we want a paragraph on outliers, given that the raincloud plot is above?", "red")`

It also assumes there to be no significant outliers that skew the results. To test this, both a Grubb's test and a Dixon's Q test were conducted. `r outliers`

#### Skewness

"Although paired t-tests are relatively robust to skewness, you could either do a transformation (e.g. log-transform), or consider to compare the results to a rank-based test"

```{r HV_skewness, include=FALSE}
skew <- skewness(data_paired$Outbursts)
if (abs(skew) >= 1) {
  skewOutput <- "The distribution of your data is highly skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you could run a different test, such as ranked-based Wilcox test"
} else if (abs(skew) > 0.5 & abs(skew) < 1) {
  skewOutput <- "The distribution of your data is moderately skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you run could a different test, such as INSERT TEST ALTERNATIVE FOR SKEWED DATA"
} else {skewOutput <- "distribution of your data is only mildly skewed, which means that you can assume normality for your data sets."}
```

Last, a skewness test was conducted to see how non-symmetric your data set is. `r skewOutput`

`r colorize("Paragraph explaining the concept of skewness and its relevance for t tests.", "red")`

#### Normality test (Shapiro-Wilk)

```{r HV_normality_test, include=FALSE}
sw_test <- shapiro.test(as.matrix(data_paired[,3]))
if (sw_test$p.value < 0.05) {
  sw_test_result <- "The Shapiro-Wilk test concluded significance, meaning that the dataset does not follow a normal distribution. Therefore you cannot conclude valid results from the t-test."
} else {
  sw_test_result <- "The Shapiro-Wilk test showed that the data does not differ significantly from normal distribution."
}
```

`r colorize("Paragraph explaining the concept of normality testing. Needs updating for text.", "red")`

#### Actual Test

`r colorize("Datasets: Is it possible to have a dataset on which one can test all types of statistical tests? Can you reasonably run an independent and paired t test on the same dataset?", "red")`

```{r all_types_of_tests, include=FALSE}
# Instead of the assumptions determining the best test, depict all tests and show which one is holding up the best when compared
# To reduce if-statements, all tests check on the variable alternative and is_paired as declared by the arguments code chunk.
if (is_paired) {
  paired_ttest <- t.test(data = data_paired, Outbursts~Condition, var.equal = TRUE, alternative = alternative, paired = is_paired)
  welch_ttest <- t.test(data = data_paired, Outbursts~Condition, var.equal = FALSE, alternative = alternative)
  paired_wilcox <- wilcox.test(data = data_paired, Outbursts~Condition, alternative = alternative, paired = is_paired)
  
# Set up for table comparison
  apa_t <- apa_print(paired_ttest)
  apa_welch <- apa_print(welch_ttest)
  apa_wilcox <- apa_print(paired_wilcox)
} else {
  ind_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = alternative)
  welch_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = FALSE, alternative = alternative)
  ind_wilcox <- wilcox.test(data = data, mean_NEO~Rotation, alternative = alternative)
  
# Set up for table comparison
  apa_t <- apa_print(ind_ttest)
  apa_welch <- apa_print(welch_ttest)
  apa_wilcox <- apa_print(ind_wilcox)
}
```

`r colorize("Have the user choose whether the alternative is two sided, less, or greater, for purpose of demonstration it's less. TABLE DOESN'T WORK YET", "red")`

```{r comparison_table, echo=FALSE}
if (is_paired) {
# Tables
  apa_table(apa_t$table, caption = "Paired t-test of the moon on outbursts")
  apa_table(apa_welch$table, caption = "Welch's t-test of the moon on outbursts")
  apa_table(apa_wilcox$table, caption = "Paired Wilcoxon signed ranks test of the moon on outbursts")
} else {
# Tables
  apa_t_table <- apa_table(apa_t$table, caption = "Independent t-test of the moon on outbursts")
  apa_table(apa_welch$table, caption = "Welch's t-test of the moon on outbursts")
  apa_table(apa_wilcox$table, caption = "Wilcoxon signed ranks test of the moon on outbursts")
}
```

```{r test_of_big_table}
apa_table(cbind(apa_t$table, apa_welch$table, apa_wilcox$table))
```

### 3. Hypothesis Testing

The analysis shows that you have `r attributes(ttest$statistic)`(`r ttest$parameter`) = `r round(ttest$statistic, 2)` with p = `r round(ttest$p.value, 4)`. This means that assuming the null hypothesis to be true, when running this test an infinite amount of times the chances of getting a score of `r round(ttest$statistic, 2)` or higher would only occur in `r round(ttest$p.value * 100, 4)`% of cases. Since `r round(ttest$p.value, 4)` is
```{r HV_signif, include=FALSE}
if (ttest$p.value < 0.05){
  signif <- "below the 0.05 threshold, the test is said to be significant and you can reject the null hypothesis."
} else {
  signif <- "above the 0.05 threshold, the test is said to be not be significant and you cannot reject the null hypothesis."}
```
`r signif`

`r colorize('Cannot just round, needs instead some evaluation like "if p < 0.001, print that"', "red")`

##### Misconceptions on Hypothesis Testing; ONLY FOR HV

Unfortunately, it is easy to have misconceptions about the p-value. In order to avoid making incorrect assertions, let's take a look at the following statements that are all incorrect: 

- The p-value states the chance of the null hypothesis being true. So if a statistical analysis gives p = 0.034, then there is a 3.4% chance of the null hypothesis being true.

Since the p-value is calculated under the assumption of the null hypothesis being true, it logically cannot give any information on whether the null hypothesis is true or not. 

- The p-value denotes the probability of a type $I$ error. So if a statistical analysis gives p = 0.034, then there is a 3.4% chance of falsely rejecting the null-hypothesis.

While this statement on the surface looks compelling, it underlies the same misconception of the previous statement. Since the p-value is calculated under the assumption of the null hypothesis being true, it cannot be linked to type $I$ or $II$ errors.

- A study with a lower p-value shows a more significant result than a study with a higher p-value. In other words, a study with p < 0.001 is more significant than a study with p < 0.05.

The p-value does not give any information on the significance of whatever was being tested. It only conveys statistical significance in the sense that the value is below a threshold (often set to 0.05), but it has no connection to effect size.

- A p-value of 0.05 means that we have observed data that would occur only 5% of the time under the null hypothesis.

This cannot be the case if you remember that the definition of the p-value is the probability of the observed data <span font-style: italic;>plus more extreme data</span> assuming the null hypothesis to be true.

### 4. Parameter Estimation

Now that you confirmed the assumptions to hold, you conducted a `r ttest$method` to test whether the `r attributes(ttest$estimate[1])` is `r ttest$alternative` than the `r attributes(ttest$estimate[2])`. To conclude this, you want to reject the null-hypothesis that assumes the true `r attributes(ttest$null.value)` to be `r ttest$null.value`.

Specifically, you test for a difference in `r ttest$data.name` with a `r as.numeric(attributes(ttest$conf.int)) * 100`% confidence interval.

##### Misconceptions about Parameter Estimation; ONLY FOR HV

In a similar vein as the misconceptions about the t-test for hypothesis testing, here are some statements of seemingly persuasive arguments which nonetheless are all false. 

- You should use a one-sided p-value when you don't care about a result in one direction, or that direction makes no conceptual sense.

This might come as a surprise, as the idea of a one-sided test seems appropriate to avoid "false" probabilities by including non-nonsensical possibilities. However, deciding on a one-sided test mathematically strengthens the evidence found in the tested direction, effectively allowing the beliefs and attitudes of the researcher to influence the statistical result. 

If you want to learn more about this issue, check out `r colorize('THIS PAPER', "red")`

#### Limitations of NHST

There exist several criticisms to this method to determine the significance of an effect. 

- it never actually tests the statistical hypothesis. 
- The threshold of 0.05 is arbitrary
- due to its method, interpretations are often incorrect as they forget/falsely assert the meaning of the probability being dependent on the null hypothesis being true
- requires blindness of the researchers on the data to avoid bias. 
- 


### 5. Sources/References

Fisher, R (1955). "Statistical Methods and Scientific Induction". Journal of the Royal Statistical Society, Series B. 17 (1): 69–78.

Neyman, J; Pearson, E. S. (January 1, 1933). "On the Problem of the most Efficient Tests of Statistical Hypotheses". Philosophical Transactions of the Royal Society A. 231 (694–706): 289–337.

#### Data

Wagenmakers, E.-J., Beek, T. F., Rotteveel, M., Gierholz, A., Matzke, D., Steingroever, H., … Pinto, Y. (2015). Turning the hands of time again: A purely confirmatory replication study and a Bayesian analysis. Frontiers in Psychology, 6.

Moore, D. S., McCabe, G. P., and Craig. B. A. (2012) Introduction to the Practice of Statistics (7th ed). New York: Freeman.

#### Tests

##### Mann-Whitney Non-Parametric Test / Wilcoxon Rank Sum

David F. Bauer (1972). Constructing confidence sets using rank statistics. Journal of the American Statistical Association 67, 687–690. doi: 10.1080/01621459.1972.10481279.

Myles Hollander and Douglas A. Wolfe (1973). Nonparametric Statistical Methods. New York: John Wiley & Sons. Pages 27–33 (one-sample), 68–75 (two-sample).
Or second edition (1999).


##### Shapiro-Wilk Normality Test

Patrick Royston (1982). An extension of Shapiro and Wilk's WW test for normality to large samples. Applied Statistics, 31, 115–124. doi:10.2307/2347973.


##### Levene's test

Fox, J. and Weisberg, S. (2019) An R Companion to Applied Regression, Third Edition, Sage.



##### Raincloud Plot

Allen, M., Poggiali, D., Whitaker, K., Marshall, T. R., & Kievit, R. A. (2019). Raincloud plots: a multi-platform tool for robust data visualization. Wellcome open research, 4, 63. https://doi.org/10.12688/wellcomeopenres.15191.1


##### Student's t-test

"Student" William Sealy Gosset (1908). "The probable error of a mean" (PDF). Biometrika. 6 (1): 1–25. doi:10.1093/biomet/6.1.1. hdl:10338.dmlcz/143545.


##### Welch's t-test

Welch, B. L. (1947). "The generalization of "Student's" problem when several different population variances are involved". Biometrika. 34 (1–2): 28–35. doi:10.1093/biomet/34.1-2.28

#### Misconceptions

Goodman, S. (2008). A Dirty Dozen: Twelve P-Value Misconceptions. Seminars in Hematology. 45 (3), 135-140. https://doi.org/10.1053/j.seminhematol.2008.04.003.

Knottnerus, J. A., & Bouter, L. M. (2001). The ethics of sample size: two-sided testing and one-sided thinking. Journal of clinical epidemiology, 54(2), 109–110. https://doi.org/10.1016/s0895-4356(00)00276-6