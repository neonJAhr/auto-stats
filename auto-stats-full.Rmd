---
title: "Auto-Stats"
author: "Arne John"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("psych")
library("outliers")
library("car")
library("e1071")
library("tidyverse")
library("papaja")
library("citr")

# For the plot
library("ggdist")
library("tidyquant")
library("ggdist")
library("ggthemes")
```

```{r colorfunction, include=FALSE}
# This function is a wrapper for an HTML-style text-color change and improves readibility
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

# 0: How to Read this Document

Currently, this is a static html page, representing the output (right side) of the JASP program. 
It is intended to become a pdf once the user hits the "stat-report" button, but for now it imitates the UI output. 


Different types of text: 

Regular text is the output found in the report, meant to be easily readable/understandable for the user.

SUPERSCRIPT DENOTES REQUIRED INPUTS or extra output not created. Additionally, [Choices] are denoted in brackets for now.

`r colorize("Red font is meant to show missing areas as reminders for me to improve on.", "red")`

## Assumptions about this document

This document assumes the user is using one of the JASP library datasets. As the report is intended to be created for different types of tests, the selected JASP module will be listed here as well.


# 1. Data Selection

☐ Directed Reading Activities & Independent Samples t-Test (significant results)

☐ Kitchen Rolls & Independent Samples t-Test (not significant results)

☑ Moon and Aggression & Paired Samples t-Test

☐ Weight Gain & One Sample t-Test

The Moon and Aggression data is selected. 

```{r data_input, include=FALSE}
data <- read.csv("kitchenRollsTTest.csv")
data_paired <- sleep
```

ONLY FOCUS ON THE PAIRED T-TEST.

<span style="color: red;">
SELECT DV AND IV.
Maybe force the IV to be [,1] and the DV to be [,2] so the following steps can just refer to those columns? Otherwise generalize it so that following tests call upon the placeholder value.</span>

### Introduction: What are you achieving with this test?

Data should follow a normal distribution, but the variance from repeating the experiment follows a t-distribution. 


Dear user,
You selected to conduct a Student's t-test. This test checks whether the data follows a Student's t-Distribution under the null hypothesis. It is often used to see whether the means of two sets of data are significantly different from one another.  

First let’s take a look at the data as a raincloud plot: 

<span style="color: red;">INSERT RAINCLOUD PLOT HERE

```{r raincloud_plot, include=FALSE}
data_paired %>% 
  # filter(group) %>% 
  ggplot(aes(x = factor(group), y = extra, fill = factor(group))) +
  
  # add half-violin from {ggdist} package
  stat_halfeye(
    # adjust bandwidth
    adjust = 0.5,
    # move to the right
    justification = -0.2,
    # remove the slub interval
    .width = 0,
    point_colour = NA
  ) +
  geom_boxplot(
    width = 0.12,
    # removing outliers
    outlier.color = NA,
    alpha = 0.5
  ) +
  stat_dots(
    # ploting on left side
    side = "left",
    # adjusting position
    justification = 1.1,
    # adjust grouping (binning) of observations
    binwidth = 0.25
  ) +
  # Themes and Labels
  scale_fill_tq() +
  theme_tq() +
  labs(
    title = "RainCloud Plot",
    x = "Condition",
    y = "Extra Sleep",
    fill = "Condition"
  ) +
  coord_flip()
```


VISUALLY CONFIRMATION OF EQUAL VARIANCE BETWEEN DATA SETS & OUTLIERS

Check for Model Misspecifications. </span>

# Low Verbosity Level

### Descriptions (Assumptions about Outliers, Skewness, Homoscedasticity)

```{r LV_homogeneity_of_variance, include = FALSE}
# Find Levene's test as code
leveneTest(data_paired$extra, group = data_paired$group)
levTest <- leveneTest(data_paired$extra, group = data_paired$group)
if (levTest$`Pr(>F)`[1] < 0.05) {
  levTestResult <- "The test showed significance, meeting that there is a difference in the variance of the two groups. That in turn means that the results of the t-test cannot be interpreted as valid. Instead of a Student's t-test, consider running a Welch's t-test which does not assume equal variance."
} else {
  levTestResult <- "The test did not show significance, meeting that the variances between the two groups is not significantly different. This means that you can safely conclude the results of the t-test."
}
```

```{r LV_test_outliers, include=FALSE}
dixon <- dixon.test(data_paired[,1])
grubbs <- grubbs.test(data_paired[,1])

if (grubbs$p.value > 0.05 & dixon$p.value > 0.05) {
  outlier_test <- TRUE
  outliers <- "The data set does not contain outliers."
  } else {
    outlier_test <- FALSE
    outliers <- cat("The data set seems to contain outliers and can therefore not be used to conclude valid results of the t-test.")
  }
```

```{r LV_normality_test, include=FALSE}
sw_test <- shapiro.test(data_paired[,1])
if (sw_test$p.value < 0.05) {
  sw_test_result <- "The Shapiro-Wilk test concluded significance, meaning that the dataset does not follow a normal distribution. Therefore you cannot conclude valid results from the t-test."
} else {
  sw_test_result <- "The Shapiro-Wilk test showed that the data does not differ significantly from normal distribution."
}
```


```{r example_test, include=FALSE}
# This code chunk needs to be before any reference to the statistic itself.

# Additionally, this ttest is there to ensure a ttest object exists (as mentioned in issue below). 
ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = "two.sided")

if (outlier_test == TRUE & levTest$`Pr(>F)`[1] <= 0.05 & sw_test$p.value >= 0.05) {
  # ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = "two.sided")
  ttest <- t.test(data = data_paired, extra~group, var.equal = TRUE, paired = TRUE, alternative = "less")
 
  ttest
  ttest$p.value
  ttest$null.value
  attributes(ttest$conf.int)
}
```

<span style="color: red;">ISSUE: This method of output requires there to be a object called ttest. </span>

Specifically, you selected to do a `r ttest$method`, with the alternative condition's mean score being`r ttest$alternative` than the control condition's.

### Actual Test

You conducted a `r ttest$method`.

#### Parameter Estimation

The estimated parameter is `r attributes(ttest$statistic)`(`r ttest$parameter`) = `r round(ttest$statistic, 2)` with p = `r round(ttest$p.value, 4)`.

```{r LV_signif, include=FALSE}
if (ttest$p.value < 0.05)
  {
  signif <- "Since your p-value was below 0.05, the test is statistically significant."
} else {
  signif <- "Since your p-value was above 0.05, the test is not statistically significant."}
```

<span style="color: red;">Which style do we use in JASP? APA style uses the term "p value", while the American Statistical Association uses "p-value".</span>
Answer: APA style

#### Hypothesis Testing

Your null-hypothesis assumed that the control condition's mean was not `r ttest$alternative` than the alternative condition.
`r signif`


# Medium Verbosity Level

```{r MV_homoscedasity_test, include = FALSE}
# Find Levene's test as code
leveneTest(data_paired$extra, group = data_paired$group)
levTest <- leveneTest(data_paired$extra, group = data_paired$group)
if (levTest$`Pr(>F)`[1] < 0.05) {
  levTestResult <- "The test showed significance, meeting that there is a difference in the variance of the two groups. That in turn means that the results of the t-test cannot be interpreted as valid. Instead of a Student's t-test, consider running a Welch's t-test which does not assume equal variance."
} else {
  levTestResult <- "The test did not show significance, meeting that the variances between the two groups is not significantly different. This means that you can safely conclude the results of the t-test."
}
```
Before you can conduct a `r ttest$method`, we first need to check for assumptions that need to be held in order for the test to be valid. The assumptions for a `r ttest$method` are as follows:

- The two sets of data have similar variance.
- There are no significant outliers in the data that could influence the results.
- Neither data set contains skewness in the distribution of results.

To test homogeneity of variance, Levene's test was performed. `r levTestResult`

```{r MV_test_outliers, include=FALSE}
dixon <- dixon.test(data_paired[,1])
grubbs <- grubbs.test(data_paired[,1])
```

```{r MV_outliers_output, include=FALSE}
if (grubbs$p.value > 0.05 & dixon$p.value > 0.05) {
  outliers <- "Both tests reported no significant effect of outliers, so you can safely report the results of the test."} else if (grubbs$p.value < 0.05 & dixon$p.value > 0.05) {
  outliers <- cat("Unfortunately, Grubb's test for outliers found a significance of the greatest outlier, G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should exclude the outlier. Don't forget to mention this in the report!")
  } else if (grubbs$p.value > 0.05 & dixon$p.value < 0.05) {
  outliers <- cat("Unfortunately, Dixon's Q test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
  } else {
  outliers <- cat("Unfortunately, both the Dixon's Q test and the Grubb's test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
}
```
It also assumes there to be no significant outliers that skew the results. To test this, both a Grubb's test and a Dixon's Q test were conducted. `r outliers`

```{r MV_skewness, include=FALSE}
skew <- skewness(data_paired$extra)
if (abs(skew) > 1) {
  skewOutput <- cat("The distribution of your data is highly skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you could run a different test, such as INSERT TEST ALTERNATIVE FOR SKEWED DATA")
} else if (abs(skew) > 0.5 & abs(skew) <= 1) {
  skewOutput <- cat("The distribution of your data is moderately skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you run could a different test, such as INSERT TEST ALTERNATIVE FOR SKEWED DATA")
} else {skewOutput <- "distribution of your data is only mildly skewed, which means that you can assume normality for your data sets."}
```
Last, a skewness test was conducted to see how non-symmetric your data set is. `r skewOutput`



## Actual Test

### Hypothesis Testing

The analysis shows that you have `r attributes(ttest$statistic)`(`r ttest$parameter`) = `r round(ttest$statistic, 2)` with p = `r round(ttest$p.value, 4)`. This means that assuming the null hypothesis to be true, when running this test an infinite amount of times the chances of getting a score of `r ttest$statistic` or higher would only occur in `r ttest$p.value * 100`% of cases. Since `r ttest$p.value` is
```{r MV_signif, include=FALSE}
if (ttest$p.value < 0.05){
  signif <- "below the 0.05 threshold, the test is said to be significant and you can reject the null hypothesis."
} else {
  signif <- "above the 0.05 threshold, the test is said to be not be significant and you cannot reject the null hypothesis."}
```
`r signif`


#### Parameter Estimation

Now that you confirmed the assumptions to hold, you conducted a `r ttest$method` to test whether the `r attributes(ttest$estimate[1])` is `r ttest$alternative` than the `r attributes(ttest$estimate[2])`. To conclude this, you want to reject the null-hypothesis that assumes the true `r attributes(ttest$null.value)` to be `r ttest$null.value`.

Specifically, you test for a difference in `r ttest$data.name` with a `r as.numeric(attributes(ttest$conf.int)) * 100`% confidence interval.




```{r, include = FALSE}
power.t.test(n=10, delta = (2.33-0.75), sd = 0.849)
```


# High Verbosity Level

### What is Null-Hypothesis Significance Testing (NHST)?

Why are we doing this? It's a comparison between means?
But you could also just estimate parameters
This procedure tests whether the group means are equal, but also the size of the difference assumption it's not zero. 

OLD VERSION: When researchers want to test a hypothesis, they usually [Is 'usually' reasonable to say? alt: In the majority of cases?] test more than one hypothesis and see how well the data is explained by each one. In classical/frequentist statistical hypothesis testing, this is often done by having one hypothesis assume that the data, which is following some form of distribution, is influenced by a certain parameter, while the other assumes this parameter to have no effect, i.e. the data's variation is only influenced by its distribution. This "other" distribution is conventionally called the null hypothesis 
<span style="color: red;">$\(H_0)$</span>
, while the hypothesis with the added parameter is referred to as the statistical/alternative hypothesis 
<span style="color: red;">$\(H_1)$</span>. 

NEW VERSION: When researchers want to test whether some property or parameter has an effect on the distribution of observed data, they conduct a null hypothesis test. In this case, the null hypothesis assumes that the parameter influencing the distribution has no effect, i.e. is equal to zero. In contrast, the alternative/statistical hypothesis calculates the size of the effect on the distribution. 

<span style="text-decoration: line-through;">
In many frequentist statistics, the way to determine significance is done via the p-value. While originally proposed by Fisher (1955) and not related to the idea of comparing hypotheses as proposed by Neyman and Pearson (1933)
</span>

Statistical significance is asserted via the p-value. This value calculated by the probability of obtaining a parameter that is at least as extreme as the observed parameter, assuming the null hypothesis to be correct/true. In other words, were one to repeat this experiment and collect data each time, the chance of getting a test result that is as high or higher as the observed result would only occur in p-value*100 percent of the time. Significance is assumed if the p-value falls below some previously asserted threshold value, usually set to 0.05, also known as the alpha level. 

<span style="color: red;">Is the word parameter here correct? Or do I need to say test result?</span>



### Descriptions / Assumptions
```{r HV_homoscedasity_test, include = FALSE}
# Assumption of Equality of Variances

# Find Levene's test as code
leveneTest(data_paired$extra, group = data_paired$group)
levTest <- leveneTest(data_paired$extra, group = data_paired$group)
if (levTest$`Pr(>F)`[1] < 0.05) {
  levTestResult <- "The test showed significance, meeting that there is a difference in the variance of the two groups. That in turn means that the results of the t-test cannot be interpreted as valid. Instead of a Student's t-test, consider running a Welch's t-test which does not assume equal variance."
} else {
  levTestResult <- "The test did not show significance, meeting that the variances between the two groups is not significantly different. This means that you can safely conclude the results of the t-test."
}

# Brown-Forsythe
bfTest <- onewaytests::bf.test(extra ~ group, data = data_paired)
if (bfTest$p.value[1] < 0.05) {
  bfTestResult <- "The test showed significance, meeting that there is a difference in the variance of the two groups. That in turn means that the results of the t-test cannot be interpreted as valid. Instead of a Student's t-test, consider running a Welch's t-test which does not assume equal variance."
} else {
  bfTestResult <- "The test did not show significance, meeting that the variances between the two groups is not significantly different. This means that you can safely conclude the results of the t-test."
}
```
Before you can conduct a `r ttest$method`, we first need to check for assumptions that need to be held in order for the test to be valid. The assumptions for a `r ttest$method` are as follows:

- The two populations in the data have similar variance.
- There are no significant outliers in the data that could influence the results.
- Neither data set contains skewness in their distribution.

<span style="color: red;">Should independence be discussed? Can we test for independence in a meaningful way, or is it just meant to be "thought-provoking?"

</span>

#### Equality of Variance / Homoscedasity

A t-test (and the other types of ANOVAs as well as regressions) assumes that although different samples can come from populations with different means, they have the same variance. Were the variances different, i.e. would the different populations have differing distributions, then you are in a sense comparing apples and oranges. <span style="color: red;">While there can be mitigating factors, such as fixed effects and similar sample sizes, different variances indicate that a t-test is an invalid method to compare these populations.</span>

To test homogeneity of variance, Levene's test was performed. `r levTestResult`

```{r HV_test_outliers, include=FALSE}
dixon <- dixon.test(data_paired[,1])
grubbs <- grubbs.test(data_paired[,1])
```

```{r HV_outliers_output, include=FALSE}
if (grubbs$p.value > 0.05 & dixon$p.value > 0.05) {
  outliers <- "Both tests reported no significant effect of outliers, so you can safely report the results of the test."} else if (grubbs$p.value < 0.05 & dixon$p.value > 0.05) {
  outliers <- cat("Unfortunately, Grubb's test for outliers found a significance of the greatest outlier, G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should exclude the outlier. Don't forget to mention this in the report!")
  } else if (grubbs$p.value > 0.05 & dixon$p.value < 0.05) {
  outliers <- cat("Unfortunately, Dixon's Q test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
  } else {
  outliers <- cat("Unfortunately, both the Dixon's Q test and the Grubb's test for outliers found a significance of the greatest outlier, Q = ", dixon$statistic[1], "stating that the", dixon$alternative,"p =", dixon$p.value,". G = ", grubbs$statistic[1], "U =", grubbs$statistic[2], "p =", grubbs$p.value,". This means that you should consider excluding the outlier. Don't forget to mention this in the report!")
}
```


#### Outliers

<span style="color: red;">Do we want a paragraph on outliers, given that the raincloud plot is above?</span>

It also assumes there to be no significant outliers that skew the results. To test this, both a Grubb's test and a Dixon's Q test were conducted. `r outliers`

#### Skewness 

"Although paired t test are relatively robust to skewness, you could either do a transfomation (e.g. log-transform), or consider to compare the results to a rank-based test"

```{r HV_skewness, include=FALSE}
skew <- skewness(data_paired$extra)
if (abs(skew) > 1) {
  skewOutput <- cat("The distribution of your data is highly skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you could run a different test, such as INSERT TEST ALTERNATIVE FOR SKEWED DATA")
} else if (abs(skew) > 0.5 & abs(skew) <= 1) {
  skewOutput <- cat("The distribution of your data is moderately skewed, which means you cannot assume a normality in your distribution. This is not necessarily a problem, as long as your data set should be sufficiently large (n per group < 20) to have a valid t-test result. Alternatively, you run could a different test, such as INSERT TEST ALTERNATIVE FOR SKEWED DATA")
} else {skewOutput <- "distribution of your data is only mildly skewed, which means that you can assume normality for your data sets."}
```
Last, a skewness test was conducted to see how non-symmetric your data set is. `r skewOutput`

<span style="color: red;">Paragraph explaining the concept of skewness and its relevance for t tests.</span>

#### Normality test (Shapiro-Wilk)

```{r HV_normality_test, include=FALSE}
sw_test <- shapiro.test(data_paired[,1])
if (sw_test$p.value < 0.05) {
  sw_test_result <- "The Shapiro-Wilk test concluded significance, meaning that the dataset does not follow a normal distribution. Therefore you cannot conclude valid results from the t-test."
} else {
  sw_test_result <- "The Shapiro-Wilk test showed that the data does not differ significantly from normal distribution."
}
```

<span style="color: red;">Paragraph explaining the concept of normality testing.

Needs updating for text.</span>

### Actual Test

<span style="color: red;">Datasets: Is it possible to have a dataset on which one can test all types of statistical tests? Can you reasonably run an independent and paired t test on the same dataset?

Don't compare paired vs independent t tests.</span>

```{r all_types_of_tests, include=FALSE}
# Instead of the assumptions determining the best test, depict all tests and show which one is holding up the best when compared

ind_twoside_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = "two.sided")
ind_less_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = "less")
ind_more_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = TRUE, alternative = "greater")

paired_twoside_ttest <- t.test(data = data_paired, extra~group, var.equal = TRUE, alternative = "two.sided", paired = TRUE)
paired_less_ttest <- t.test(data = data_paired, extra~group, var.equal = TRUE, alternative = "less", paired = TRUE)
paired_more_ttest <- t.test(data = data_paired, extra~group, var.equal = TRUE, alternative = "greater", paired = TRUE)

welch_twoside_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = FALSE, alternative = "two.sided")
welch_less_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = FALSE, alternative = "less")
welch_more_ttest <- t.test(data = data, mean_NEO~Rotation, var.equal = FALSE, alternative = "greater")

ind_twoside_wilcox <- wilcox.test(data = data, mean_NEO~Rotation, alternative = "two.sided")
ind_less_wilcox <- wilcox.test(data = data, mean_NEO~Rotation, alternative = "less")
ind_more_wilcox <- wilcox.test(data = data, mean_NEO~Rotation, alternative = "greater")

paired_twoside_wilcox <- wilcox.test(data = data_paired, extra~group, alternative = "two.sided", paired = TRUE)
paired_less_wilcox <- wilcox.test(data = data_paired, extra~group, alternative = "less", paired = TRUE)
paired_more_wilcox <- wilcox.test(data = data_paired, extra~group, alternative = "greater", paired = TRUE)
```

<span style="color: red;">Have the user choose whether the alternative is two sided, less, or greater, for purpose of demonstration it's less. TABLE DOESN'T WORK YET</span>
```{r comparison_table, include=FALSE}
# anova(ind_less_ttest,paired_less_ttest,welch_less_ttest,ind_less_wilcox,paired_less_wilcox)
```

### Hypothesis Testing

The analysis shows that you have `r attributes(ttest$statistic)`(`r ttest$parameter`) = `r round(ttest$statistic, 2)` with p = `r round(ttest$p.value, 4)`. This means that assuming the null hypothesis to be true, when running this test an infinite amount of times the chances of getting a score of `r ttest$statistic` or higher would only occur in `r ttest$p.value * 100`% of cases. Since `r ttest$p.value` is
```{r HV_signif, include=FALSE}
if (ttest$p.value < 0.05){
  signif <- "below the 0.05 threshold, the test is said to be significant and you can reject the null hypothesis."
} else {
  signif <- "above the 0.05 threshold, the test is said to be not be significant and you cannot reject the null hypothesis."}
```
`r signif`

##### Misconceptions on Hypothesis Testing; ONLY FOR HV

#### Parameter Estimation
Now that you confirmed the assumptions to hold, you conducted a `r ttest$method` to test whether the `r attributes(ttest$estimate[1])` is `r ttest$alternative` than the `r attributes(ttest$estimate[2])`. To conclude this, you want to reject the null-hypothesis that assumes the true `r attributes(ttest$null.value)` to be `r ttest$null.value`.

Specifically, you test for a difference in `r ttest$data.name` with a `r as.numeric(attributes(ttest$conf.int)) * 100`% confidence interval.

##### Misconceptiions about Parameter Estimation; ONLY FOR HV

INSTEAD OF ITS OWN SECTION
### Limitations of NHST

There exist several criticisms to this method to determine the significance of an effect. 

- it never actually tests the statistical hypothesis. 
- The threshold of 0.05 is arbitrary
- due to its method, interpretations are often incorrect as they forget/falsely assert the meaning of the probability being dependent on the null hypothesis being true
- requires blindness of the researchers on the data to avoid bias. 

RESCHEDULE NEXT WEEK TO THURSDAY
If publicly available, then 

# Sources/References

Fisher, R (1955). "Statistical Methods and Scientific Induction". Journal of the Royal Statistical Society, Series B. 17 (1): 69–78.

Neyman, J; Pearson, E. S. (January 1, 1933). "On the Problem of the most Efficient Tests of Statistical Hypotheses". Philosophical Transactions of the Royal Society A. 231 (694–706): 289–337. 

## Tests

<span style="color: red;">Raincloud Plots Paper</span>

Mann-Whitney Non-Parametric Test / Wilcoxon Rank Sum
David F. Bauer (1972). Constructing confidence sets using rank statistics. Journal of the American Statistical Association 67, 687–690. doi: 10.1080/01621459.1972.10481279.

Myles Hollander and Douglas A. Wolfe (1973). Nonparametric Statistical Methods. New York: John Wiley & Sons. Pages 27–33 (one-sample), 68–75 (two-sample).
Or second edition (1999).


Shapiro-Wilk Normality Test
Patrick Royston (1982). An extension of Shapiro and Wilk's WW test for normality to large samples. Applied Statistics, 31, 115–124. doi:10.2307/2347973.


Levene's test
Fox, J. and Weisberg, S. (2019) An R Companion to Applied Regression, Third Edition, Sage.


Student's t-test
"Student" William Sealy Gosset (1908). "The probable error of a mean" (PDF). Biometrika. 6 (1): 1–25. doi:10.1093/biomet/6.1.1. hdl:10338.dmlcz/143545.


Welch's t-test
Welch, B. L. (1947). "The generalization of "Student's" problem when several different population variances are involved". Biometrika. 34 (1–2): 28–35. doi:10.1093/biomet/34.1-2.28

